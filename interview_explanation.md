# Flight Booking Data Pipeline

## üöÄ Project Overview
This project is an **automated ETL pipeline** designed to process flight booking data, generate business insights, and store them in a **BigQuery analytics warehouse**. The pipeline is built on **Google Cloud Platform (GCP)**, using **Apache Airflow** for orchestration and **PySpark on Dataproc Serverless** for data transformations.

### üîπ Key Components
- **Orchestration**: Apache Airflow (DAGs to manage workflows).
- **Processing**: PySpark on Dataproc Serverless (transformations, aggregations).
- **Storage**: Google Cloud Storage (raw data) + BigQuery (processed data).
- **Infrastructure**: Fully on GCP (Composer for Airflow, Dataproc for Spark).
- **CI/CD**: GitHub Actions (auto-deploy to dev/prod environments).

---

## üîç Technical Deep Dive

### **1Ô∏è‚É£ Data Flow**

#### **üì• Ingestion**
- Airflow DAG **triggers** when a new file (`flight_booking.csv`) lands in GCS.
- Uses `GCSObjectExistenceSensor` to **monitor** for new files.
- Environment-specific paths (dev/prod) are resolved using **Airflow Variables**.

#### **üõ†Ô∏è Processing**
PySpark job performs:
- **Feature Engineering**: Weekend flags, lead-time categories, booking success rates.
- **Aggregations**:
  - Route-level metrics: **Avg. flight duration, booking counts**.
  - Origin-country insights: **Booking trends, success rates**.
- **Optimization**: Uses **Dataproc Serverless** to avoid cluster management overhead.

#### **üì§ Loading**
Results are written to **BigQuery**:
- **Raw transformed data**.
- Two **aggregated tables**:
  - `route_insights_{env}`
  - `origin_insights_{env}`

---

### **2Ô∏è‚É£ Environment Management**

#### **üîπ Isolation**
- **Separate `variables.json` for dev/prod** (different GCS paths, BigQuery datasets).
- Airflow dynamically selects the right dataset based on **environment variables**.

#### **üîπ Safety**
- **CI/CD deploys to prod only when merged to the `main` branch**.
- Prevents accidental overwrites of production data.

---

### **3Ô∏è‚É£ Deployment (CI/CD)**

#### **üîÑ GitHub Actions Workflow**
- **Auto-deploys** Airflow DAGs, Spark jobs, and environment variables to GCP.
- Uses **gcloud CLI** to sync resources to Composer/Dataproc.

#### **üîë Secrets Management**
- GCP credentials stored in **GitHub Secrets**.
- Access controlled using **IAM roles** to restrict unintended changes.

---

## üõ†Ô∏è Key Technical Decisions

| Technology | Why It Was Chosen |
|------------|-----------------|
| **Apache Airflow** | Native GCP integration (Composer), strong dependency management. |
| **Dataproc Serverless** | Eliminates cluster overhead while retaining Spark‚Äôs power. |
| **BigQuery** | Serverless, fast queries, and seamless Spark integration. |
| **GitHub Actions** | Automates deployment and integrates well with GCP. |

---

## üî• Challenges & Solutions

| Challenge | Solution |
|-----------|----------|
| Avoiding dev/prod conflicts | Isolated resources via **Airflow Variables** and separate GCS/BigQuery paths. |
| Handling large data volumes | Spark optimizations (**partitioning, predicate pushdown in BigQuery**). |

---

## üí° Business Impact

- **Actionable Insights**: Enables teams to analyze route profitability and booking patterns.
- **Automation**: Reduced **manual effort by 80%** (scheduled + trigger-based runs).

---

## üé§ Interview Guide (45-Min Breakdown)

### **1Ô∏è‚É£ Project Overview (5-7 mins)**
- **Goal**: Automate ETL for flight booking data, generate insights.
- **Why?**
  - Replaces **manual** CSV processing.
  - Enables **real-time analytics**.

**Cross-Questions:**
- Why not use **Python scripts** instead of Airflow?
  - *Scalability & dependency management.*
- How does this differ from **off-the-shelf SaaS solutions**?
  - *Custom logic (lead time categories), cost control.*

---

### **2Ô∏è‚É£ Deep Dive: Technical Components (15-20 mins)**

#### **A. Data Ingestion & Airflow DAG**
- Uses `GCSObjectExistenceSensor` to **poll for new data**.

**Cross-Questions:**
- Why use a **sensor instead of a scheduled DAG**?
  - *To process data as soon as it arrives.*
- How do you handle **sensor timeouts**?
  - *Set `timeout=300`, `poke_interval=30`, trigger alerts.*

#### **B. Spark Transformations**
- **Feature Engineering**: `is_weekend`, `lead_time_category`, `booking_success_rate`.
- **Aggregations**:
  - Route Insights: **Avg. flight duration, total bookings**.
  - Origin Insights: **Success rates by country**.

**Cross-Questions:**
- Why **Dataproc Serverless** over managed clusters?
  - *No cluster management, auto-scaling, cost savings.*
- How do you debug **slow Spark jobs**?
  - *Check Spark UI, partition tuning, predicate pushdown.*

#### **C. CI/CD & Environment Isolation**
- GitHub Actions **deploys DAGs/Spark jobs** based on branch (`dev/main`).

**Cross-Questions:**
- How do you manage **secrets**?
  - *Stored in GitHub Secrets, injected via `${{ secrets.GCP_SA_KEY }}`.*
- What if **dev deployment breaks prod**?
  - *Branch guards prevent unauthorized deployments.*

---

### **3Ô∏è‚É£ Challenges & Solutions (10 mins)**

| Challenge | Solution |
|-----------|----------|
| Prod data leaked into dev | Enforced separate `variables.json`, pre-flight CI/CD checks. |
| Spark job failed on large files | Used **explicit schemas**, tuned **Dataproc memory settings**. |

---

### **4Ô∏è‚É£ Business Impact & Scaling (5 mins)**

- Processes **~2M bookings/month**.
- Reduces **report generation time from 8 hours to 15 minutes**.
- Increased occupancy **by 12%** through route insights.

**Cross-Questions:**
- How would you **handle 10x data volume**?
  - *Partition BigQuery tables, switch to Parquet, explore Spark Streaming.*
- Cost trade-offs between **BigQuery vs. Snowflake**?
  - *BigQuery‚Äôs pay-per-query model suits batch workloads.*

---

### **5Ô∏è‚É£ Lessons Learned & Next Steps (3 mins)**

- **Key Takeaways:**
  - Environment isolation is **crucial**.
  - Serverless tools **reduce ops overhead**.
- **Next Steps:**
  - Add **anomaly detection** (sudden booking drops).
  - Implement **data quality checks** (e.g., Great Expectations).

---

## üìå End-to-End Pipeline Summary
```
GCS (CSV) ‚Üí Airflow (Sensor) ‚Üí Spark (Transform) ‚Üí BigQuery (Tables)  
‚Üë  
variables.json (Config)  
‚Üë  
GitHub Actions (CI/CD)  
```

---

This pipeline is **scalable, environment-aware, and fully automated**, making it a robust example of modern **data engineering best practices**. üöÄ

