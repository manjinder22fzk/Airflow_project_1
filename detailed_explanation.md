# Flight Booking Data Pipeline - End-to-End Breakdown

## ğŸ“¥ 1. Data Ingestion & Trigger
**Source:** Flight booking data (e.g., CSV files like `flight_booking.csv`) lands in **Google Cloud Storage (GCS)**.  
ğŸ“Œ **Example path:** `gs://airflow-projetcs-gds/airflow-project-1/source-dev/flight_booking.csv`.

### ğŸ”„ Trigger:
- **Airflowâ€™s `GCSObjectExistenceSensor`** monitors the GCS path for new files.
- Waits indefinitely (with timeouts) until the file appears, then proceeds.

---

## âš™ï¸ 2. Environment Configuration
### ğŸ”§ Dynamic Settings:
- Airflow pulls environment-specific variables from `variables.json` (e.g., dev vs. prod).

ğŸ“Œ **Example variables:**
```json
{
  "env": "dev",
  "gcs_bucket": "airflow-projetcs-gds",
  "bq_dataset": "flight_data_dev",
  "tables": {
    "transformed_table": "transformed_flight_data_dev",
    "route_insights_table": "route_insights_dev"
  }
}
```
### ğŸ”’ Isolation:
- Separate **GCS paths**, **BigQuery datasets**, and **table names** for dev/prod to avoid conflicts.

---

## ğŸ”¥ 3. Data Processing (PySpark on Dataproc Serverless)
### ğŸš€ Job Submission:
- **Airflowâ€™s `DataprocCreateBatchOperator`** submits a PySpark job (`spark_transformation_job.py`) to **Dataproc Serverless**.

ğŸ“Œ **Arguments passed:**
```python
args=[
  "--env=dev",
  "--bq_project=my-project",
  "--bq_dataset=flight_data_dev",
  "--transformed_table=transformed_flight_data_dev",
  "--route_insights_table=route_insights_dev"
]
```

### ğŸ” Transformations:
#### **Feature Engineering:**
- Adds columns like **`is_weekend`** (binary flag for weekend flights).
- Categorizes **lead_time** into "Last-Minute" / "Short-Term" / "Long-Term".
- Calculates **`booking_success_rate`** (successful bookings per passenger).

#### **Aggregations:**
- **Route Insights:** Avg. flight duration, stay length, and booking counts per route.
- **Origin Insights:** Booking success rates and purchase trends by country.

---

## ğŸ“Š 4. Data Loading (BigQuery)
### ğŸ“¥ **Output Tables:**
#### **Transformed Raw Data:**
- **Schema:** Original columns + derived features (e.g., `is_weekend`).
- **Table:** `transformed_flight_data_{env}`.

#### **Route Insights:**
- **Metrics:** route, avg_flight_duration, total_bookings.
- **Table:** `route_insights_{env}`.

#### **Origin Insights:**
- **Metrics:** booking_origin, success_rate, avg_purchase_lead.
- **Table:** `origin_insights_{env}`.

### ğŸš€ **Loading Method:**
- Direct write to BigQuery using **Sparkâ€™s native connector** (`format("bigquery")`).
- **Overwrite mode:** Each run refreshes tables (**idempotent**).

---

## ğŸš€ 5. Deployment & CI/CD (GitHub Actions)
### ğŸ”„ **Workflow:**
#### **Dev Branch (`dev`) â†’ Deploys to dev environment:**
- Uploads `variables/dev/variables.json` to GCS.
- Syncs Spark job (`spark_transformation_job.py`) to GCS.
- Deploys Airflow DAG (`airflow_job.py`) to Composerâ€™s `dags/` folder.

#### **Prod Branch (`main`) â†’ Same steps, but for prod (isolated resources).**

### ğŸ” **Security:**
- **GCP credentials stored in GitHub Secrets** (`GCP_SA_KEY`, `GCP_PROJECT_ID`).

---

## âš ï¸ 6. Error Handling & Observability
### ğŸ” **Airflow:**
- Retries failed tasks (**e.g., Spark job fails â†’ auto-retry after 5 mins**).
- Logs stored in **GCS or Cloud Logging**.

### ğŸ“Š **Spark:**
- Structured logging (**e.g., `logger.info("Data read from GCS")`**).
- Alerts on exceptions (**e.g., `try-catch` blocks with `sys.exit(1)`)**.

---

## ğŸ¯ 7. Business Impact
### ğŸ“ˆ **Insights Delivered:**
- **Operations Team:** Identify peak booking times, underperforming routes.
- **Marketing Team:** Target high-potential origin countries.

### ğŸ’° **Cost Savings:**
- **Dataproc Serverless** reduces cluster costs by **70% vs. managed clusters**.
- **Automated pipeline** saves **~10 hours/week** of manual effort.

---

## ğŸ›  Key Tools & Why They Were Chosen
| Tool                 | Role               | Why?  |
|----------------------|-------------------|------------------------------------------------|
| **Apache Airflow**   | Orchestration     | Native GCP integration (Composer), dependency management. |
| **PySpark (Dataproc)** | Data Processing  | Serverless, scalable, and cost-efficient for large datasets. |
| **BigQuery**         | Analytics Storage | Serverless, fast SQL, and Spark connector. |
| **GitHub Actions**   | CI/CD             | Free, integrates with GitHub, and supports GCP auth. |

---

## ğŸ”— End-to-End Flow Summary
```plaintext
GCS (CSV) â†’ Airflow (Sensor) â†’ Spark (Transform) â†’ BigQuery (Tables)  
                â†‘  
        variables.json (Config)  
                â†‘  
       GitHub Actions (CI/CD)  
```

This pipeline is **reproducible, scalable, and environment-aware**, making it a **robust example of modern data engineering practices**. Let me know if you'd like to dive deeper into any section! ğŸš€
